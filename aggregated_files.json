Current code below:
[
  {
    "filename": "routes.py",
    "language": "Python",
    "content": "from typing import Dict, List, Optional, Set, Any\nfrom fastapi import APIRouter, Depends, HTTPException, Request, status\nfrom uuid import UUID\n\nfrom app.core.config import Settings, get_settings\nfrom app.models.query import QueryRequest, QueryResponse, DocumentRequest, DocumentResponse\nfrom app.services import VectorSearchService\nfrom app.utils.logger import get_logger\n\nlogger = get_logger(\"api_routes\")\n\nrouter = APIRouter(prefix=\"/api\")\n\n\nasync def get_tenant_id(request: Request) -> Optional[str]:\n    \"\"\"\n    Extract tenant ID from request.\n    \n    In a production app, this would validate the token and extract the tenant ID.\n    For now, we're using a header or query param as a placeholder.\n    \"\"\"\n    tenant_id = request.headers.get(\"X-Tenant-ID\")\n    \n    if not tenant_id:\n        tenant_id = request.query_params.get(\"tenant_id\")\n    \n    if not tenant_id:\n        # For development, use a default tenant\n        if not get_settings().ENABLE_AUTH:\n            tenant_id = \"default_tenant\"\n            logger.warning(\"Using default tenant ID as authentication is disabled\")\n    \n    return tenant_id\n\n\n@router.post(\"/query\", response_model=QueryResponse)\nasync def query(\n    query_request: QueryRequest,\n    settings: Settings = Depends(get_settings),\n):\n    \"\"\"\n    Perform a vector search query.\n    \n    Query the vector store with the given text and return the most relevant matches.\n    \n    Args:\n        query_request: The query request with text and optional filters\n        \n    Returns:\n        A query response with matches\n    \"\"\"\n    try:\n        # Create vector search service\n        vector_search_service = VectorSearchService(settings)\n        \n        # Get tenant ID from query or use None\n        tenant_id = query_request.tenant_id\n        \n        # Perform query\n        response = await vector_search_service.query(\n            query_text=query_request.query,\n            top_k=query_request.top_k,\n            tenant_id=tenant_id,\n            filters=query_request.filters,\n            min_score=query_request.min_score,\n            include_content=query_request.include_content\n        )\n        \n        if not response.matches:\n            # Return empty response, not an error\n            logger.warning(f\"No matches found for query: {query_request.query}\")\n            return response\n        \n        return response\n    except Exception as e:\n        logger.error(f\"Error processing query: {str(e)}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Error processing query: {str(e)}\",\n        )\n\n\n@router.get(\"/doc/{document_id}\", response_model=DocumentResponse)\nasync def get_document(\n    document_id: str,\n    request: DocumentRequest = Depends(),\n    settings: Settings = Depends(get_settings),\n):\n    \"\"\"\n    Get document by ID.\n    \n    Retrieve document metadata and content chunks by ID.\n    \n    Args:\n        document_id: The document ID\n        request: Document request with optional tenant ID\n        \n    Returns:\n        A document response with metadata and chunks\n    \"\"\"\n    try:\n        # Create vector search service\n        vector_search_service = VectorSearchService(settings)\n        \n        # Get document\n        document = await vector_search_service.get_document(\n            document_id=document_id,\n            tenant_id=request.tenant_id\n        )\n        \n        if not document:\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND,\n                detail=f\"Document {document_id} not found\",\n            )\n        \n        return document\n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error retrieving document {document_id}: {str(e)}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Error retrieving document: {str(e)}\",\n        )\n\n\n@router.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\"status\": \"ok\", \"service\": \"Juris Query Service\"}"
  },
  {
    "filename": "config.py",
    "language": "Python",
    "content": "from functools import lru_cache\nfrom typing import List, Optional, Set, Union\n\nfrom pydantic import AnyHttpUrl, validator\nfrom pydantic_settings import BaseSettings\nimport dotenv\ndotenv.load_dotenv()\n\nclass Settings(BaseSettings):\n    # API Settings\n    API_HOST: str = \"0.0.0.0\"\n    API_PORT: int = 8002  # Different from other services\n    API_DEBUG: bool = False\n    API_SECRET_KEY: str\n    API_ALLOWED_ORIGINS: str = \"http://localhost:3000\"\n\n    # Azure Cognitive Search\n    AZURE_SEARCH_ENDPOINT: str\n    AZURE_SEARCH_KEY: str\n    AZURE_SEARCH_INDEX_NAME: str = \"vector1\"\n    AZURE_SEARCH_VECTOR_FIELD: str = \"vector_content\"\n    AZURE_SEARCH_TEXT_FIELD: str = \"content\"\n    \n    # Query Settings\n    DEFAULT_TOP_K: int = 5\n    MIN_SIMILARITY_SCORE: float = 0.7\n    \n    # Azure OpenAI (for embedding queries)\n    AZURE_OPENAI_ENDPOINT: str\n    AZURE_OPENAI_KEY: str\n    AZURE_OPENAI_EMBEDDING_DEPLOYMENT: str = \"text-embedding-ada-002\"\n    AZURE_OPENAI_EMBEDDING_MODEL: str = \"text-embedding-ada-002\"\n    AZURE_OPENAI_EMBEDDING_DIMS: int = 1536\n    \n    # Logging\n    LOG_LEVEL: str = \"INFO\"\n\n    # Feature Flags\n    ENABLE_AUTH: bool = False  # Will be enabled in Stage 5\n    \n    # Computed properties\n    ALLOWED_ORIGINS: List[AnyHttpUrl] = []\n\n    @validator(\"ALLOWED_ORIGINS\", pre=True)\n    def assemble_cors_origins(cls, v: Optional[str]) -> List[str]:\n        if isinstance(v, str) and not v.startswith(\"[\"):\n            return [origin.strip() for origin in v.split(\",\")]\n        elif isinstance(v, (list, str)):\n            return v\n        return []\n\n    class Config:\n        env_file = \".env\"\n        case_sensitive = True\n\n\n@lru_cache()\ndef get_settings() -> Settings:\n    return Settings()"
  },
  {
    "filename": "query.py",
    "language": "Python",
    "content": "from datetime import datetime\nfrom enum import Enum\nfrom typing import Dict, List, Optional, Set, Union, Any\nfrom uuid import UUID, uuid4\n\nfrom pydantic import BaseModel, Field, validator\n\n\nclass MetadataFilter(BaseModel):\n    \"\"\"Model for metadata filters.\"\"\"\n    field: str\n    value: Union[str, List[str], int, float, bool]\n    operator: str = \"eq\"  # eq, ne, gt, lt, gte, lte, in\n\n\nclass QueryRequest(BaseModel):\n    \"\"\"Model for a vector search query request.\"\"\"\n    query: str\n    top_k: Optional[int] = None\n    tenant_id: Optional[str] = None\n    filters: Optional[List[MetadataFilter]] = None\n    min_score: Optional[float] = None\n    include_content: bool = True\n    \n\nclass VectorMatch(BaseModel):\n    \"\"\"Model for a vector search match result.\"\"\"\n    id: str\n    score: float\n    content: Optional[str] = None\n    document_id: str\n    chunk_id: str\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n\n\nclass QueryResponse(BaseModel):\n    \"\"\"Model for a vector search query response.\"\"\"\n    matches: List[VectorMatch]\n    total_matches: int\n    query: str\n    query_time_ms: float\n\n\nclass DocumentRequest(BaseModel):\n    \"\"\"Model for a document request.\"\"\"\n    tenant_id: Optional[str] = None\n\n\nclass DocumentChunk(BaseModel):\n    \"\"\"Model for a document chunk.\"\"\"\n    chunk_id: str\n    document_id: str\n    content: str\n    chunk_index: int\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n\n\nclass DocumentResponse(BaseModel):\n    \"\"\"Model for a document response.\"\"\"\n    document_id: str\n    chunks: List[DocumentChunk]\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    total_chunks: int"
  },
  {
    "filename": "azure_search.py",
    "language": "Python",
    "content": "import json\nimport time\nfrom typing import Dict, List, Optional, Any, Set\nimport httpx\nimport asyncio\nimport json\n\nfrom app.core.config import Settings, get_settings\nfrom app.models.query import VectorMatch, MetadataFilter\nfrom app.utils.logger import get_logger\n\nlogger = get_logger(\"azure_search\")\n\n\nclass AzureCognitiveSearchService:\n    \"\"\"Service for interacting with Azure Cognitive Search.\"\"\"\n    \n    def __init__(self, settings: Settings = None):\n        self.settings = settings or get_settings()\n        self.endpoint = self.settings.AZURE_SEARCH_ENDPOINT\n        self.api_key = self.settings.AZURE_SEARCH_KEY\n        self.index_name = self.settings.AZURE_SEARCH_INDEX_NAME\n        self.vector_field = self.settings.AZURE_SEARCH_VECTOR_FIELD\n        self.text_field = self.settings.AZURE_SEARCH_TEXT_FIELD\n        self.api_version = \"2023-07-01-Preview\"\n        \n        # Clean endpoint if it has a trailing slash\n        if self.endpoint.endswith(\"/\"):\n            self.endpoint = self.endpoint[:-1]\n            \n        # Log initialization\n        logger.info(f\"Initializing Azure Search service with endpoint: {self.endpoint}\")\n        logger.info(f\"Using index name: '{self.index_name}'\")\n        logger.info(f\"Vector field: {self.vector_field}\")\n        logger.info(f\"Text field: {self.text_field}\")\n        \n    async def query_vector(\n        self, \n        vector: List[float],\n        top_k: int = 5,\n        tenant_id: Optional[str] = None,\n        filters: Optional[List[MetadataFilter]] = None,\n        min_score: Optional[float] = None,\n        include_content: bool = True\n    ) -> List[VectorMatch]:\n        \"\"\"\n        Query the vector search index.\n        \n        Args:\n            vector: The query vector (embedding)\n            top_k: The number of top matches to return\n            tenant_id: Optional tenant ID for filtering\n            filters: Optional list of metadata filters\n            min_score: Optional minimum similarity score threshold\n            include_content: Whether to include content in the response\n            \n        Returns:\n            A list of vector matches\n        \"\"\"\n        try:\n            # Construct the URL\n            url = f\"{self.endpoint}/indexes/{self.index_name}/docs/search?api-version={self.api_version}\"\n            \n            # Prepare headers\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"api-key\": self.api_key,\n            }\n            \n            # Build filters\n            filter_expression = None\n            if tenant_id or filters:\n                filter_parts = []\n                \n                # Add tenant_id filter if provided\n                if tenant_id:\n                    filter_parts.append(f\"content_metadata/tenant_id eq '{tenant_id}'\")\n                \n                # Add other filters\n                if filters:\n                    for f in filters:\n                        if f.operator == \"eq\":\n                            if isinstance(f.value, str):\n                                filter_parts.append(f\"content_metadata/{f.field} eq '{f.value}'\")\n                            else:\n                                filter_parts.append(f\"content_metadata/{f.field} eq {f.value}\")\n                        elif f.operator == \"ne\":\n                            if isinstance(f.value, str):\n                                filter_parts.append(f\"content_metadata/{f.field} ne '{f.value}'\")\n                            else:\n                                filter_parts.append(f\"content_metadata/{f.field} ne {f.value}\")\n                        elif f.operator == \"gt\":\n                            filter_parts.append(f\"content_metadata/{f.field} gt {f.value}\")\n                        elif f.operator == \"lt\":\n                            filter_parts.append(f\"content_metadata/{f.field} lt {f.value}\")\n                        elif f.operator == \"gte\":\n                            filter_parts.append(f\"content_metadata/{f.field} ge {f.value}\")\n                        elif f.operator == \"lte\":\n                            filter_parts.append(f\"content_metadata/{f.field} le {f.value}\")\n                        elif f.operator == \"in\" and isinstance(f.value, list):\n                            in_parts = []\n                            for v in f.value:\n                                if isinstance(v, str):\n                                    in_parts.append(f\"content_metadata/{f.field} eq '{v}'\")\n                                else:\n                                    in_parts.append(f\"content_metadata/{f.field} eq {v}\")\n                            if in_parts:\n                                filter_parts.append(f\"({' or '.join(in_parts)})\")\n                \n                # Combine all filters with AND\n                if filter_parts:\n                    filter_expression = \" and \".join(filter_parts)\n            \n            # Build the request\n            request_payload = {\n                \"vectorQueries\": [\n                    {\n                        \"vector\": vector,\n                        \"fields\": self.vector_field,\n                        \"k\": top_k,\n                        \"kind\": \"vector\"\n                    }\n                ],\n                \"top\": top_k\n            }\n            \n            # Add filter if exists\n            if filter_expression:\n                request_payload[\"filter\"] = filter_expression\n                \n            # Select fields\n            select_fields = [\"id\", \"score\"]\n            if include_content:\n                select_fields.append(self.text_field)\n            select_fields.append(\"content_metadata\")\n            \n            request_payload[\"select\"] = \",\".join(select_fields)\n            \n            # Log search parameters\n            logger.info(f\"Searching Azure Search index '{self.index_name}' for top {top_k} vectors\")\n            if filter_expression:\n                logger.info(f\"Using filter: {filter_expression}\")\n            \n            # Make the request\n            async with httpx.AsyncClient() as client:\n                start_time = time.time()\n                response = await client.post(\n                    url,\n                    headers=headers,\n                    json=request_payload,\n                    timeout=60.0\n                )\n                end_time = time.time()\n                \n                # Check the response\n                if response.status_code != 200:\n                    logger.error(f\"Error querying vector index: HTTP {response.status_code}\")\n                    logger.error(f\"Response: {response.text}\")\n                    return []\n                \n                # Parse the results\n                result = response.json()\n                matches = []\n                \n                for match in result.get(\"value\", []):\n                    # Extract score\n                    score = match.get(\"@search.score\", 0.0)\n                    \n                    # Skip results below score threshold\n                    if min_score is not None and score < min_score:\n                        continue\n                    \n                    # Extract metadata from content_metadata field\n                    metadata = {}\n                    try:\n                        if \"content_metadata\" in match:\n                            metadata_raw = match[\"content_metadata\"]\n                            if isinstance(metadata_raw, str):\n                                metadata = json.loads(metadata_raw)\n                            elif isinstance(metadata_raw, dict):\n                                metadata = metadata_raw\n                    except (json.JSONDecodeError, TypeError):\n                        logger.warning(f\"Failed to parse metadata for document {match.get('id')}\")\n                    \n                    # Extract required IDs and fields\n                    document_id = metadata.get(\"document_id\", \"unknown\")\n                    chunk_id = metadata.get(\"chunk_id\", \"unknown\")\n                    \n                    # Create vector match result\n                    vector_match = VectorMatch(\n                        id=match.get(\"id\", \"\"),\n                        score=score,\n                        document_id=document_id,\n                        chunk_id=chunk_id,\n                        metadata=metadata\n                    )\n                    \n                    # Add content if requested\n                    if include_content and self.text_field in match:\n                        vector_match.content = match[self.text_field]\n                    \n                    matches.append(vector_match)\n                \n                # Log the timing and count\n                query_time_ms = (end_time - start_time) * 1000\n                logger.info(f\"Vector search completed in {query_time_ms:.2f}ms, found {len(matches)} matches\")\n                \n                return matches\n        except Exception as e:\n            logger.error(f\"Error performing vector search: {str(e)}\")\n            return []\n    \n    async def get_document_chunks(self, document_id: str, tenant_id: Optional[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get all chunks for a specific document.\n        \n        Args:\n            document_id: The document ID\n            tenant_id: Optional tenant ID for filtering\n            \n        Returns:\n            A list of document chunks\n        \"\"\"\n        try:\n            # Construct the URL\n            url = f\"{self.endpoint}/indexes/{self.index_name}/docs/search?api-version={self.api_version}\"\n            \n            # Prepare headers\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"api-key\": self.api_key,\n            }\n            \n            # Build filter\n            filter_expression = f\"content_metadata/document_id eq '{document_id}'\"\n            if tenant_id:\n                filter_expression += f\" and content_metadata/tenant_id eq '{tenant_id}'\"\n            \n            # Build the request\n            request_payload = {\n                \"search\": \"*\",\n                \"filter\": filter_expression,\n                \"select\": f\"id,{self.text_field},content_metadata\",\n                \"top\": 100,  # Retrieve up to 100 chunks\n                \"orderby\": \"content_metadata/chunk_index\"\n            }\n            \n            # Make the request\n            async with httpx.AsyncClient() as client:\n                response = await client.post(\n                    url,\n                    headers=headers,\n                    json=request_payload,\n                    timeout=60.0\n                )\n                \n                # Check the response\n                if response.status_code != 200:\n                    logger.error(f\"Error getting document chunks: HTTP {response.status_code}\")\n                    logger.error(f\"Response: {response.text}\")\n                    return []\n                \n                # Parse the results\n                result = response.json()\n                chunks = []\n                \n                for chunk in result.get(\"value\", []):\n                    # Extract metadata\n                    metadata = {}\n                    try:\n                        if \"content_metadata\" in chunk:\n                            metadata_raw = chunk[\"content_metadata\"]\n                            if isinstance(metadata_raw, str):\n                                metadata = json.loads(metadata_raw)\n                            elif isinstance(metadata_raw, dict):\n                                metadata = metadata_raw\n                    except (json.JSONDecodeError, TypeError):\n                        logger.warning(f\"Failed to parse metadata for chunk {chunk.get('id')}\")\n                    \n                    # Create chunk result\n                    chunk_result = {\n                        \"id\": chunk.get(\"id\", \"\"),\n                        \"document_id\": metadata.get(\"document_id\", \"\"),\n                        \"chunk_id\": metadata.get(\"chunk_id\", \"\"),\n                        \"chunk_index\": metadata.get(\"chunk_index\", 0),\n                        \"metadata\": metadata\n                    }\n                    \n                    # Add content if available\n                    if self.text_field in chunk:\n                        chunk_result[\"content\"] = chunk[self.text_field]\n                    \n                    chunks.append(chunk_result)\n                \n                return chunks\n        except Exception as e:\n            logger.error(f\"Error retrieving document chunks: {str(e)}\")\n            return []"
  },
  {
    "filename": "logger.py",
    "language": "Python",
    "content": "import logging\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom app.core.config import get_settings\n\n\ndef get_logger(name: str, log_level: Optional[str] = None) -> logging.Logger:\n    \"\"\"\n    Create a logger with the specified name and log level.\n    \n    Args:\n        name: The name of the logger.\n        log_level: The log level. If not provided, it will be taken from settings.\n        \n    Returns:\n        A configured logger.\n    \"\"\"\n    settings = get_settings()\n    level = log_level or settings.LOG_LEVEL\n    \n    # Convert string log level to numeric value\n    numeric_level = getattr(logging, level.upper(), None)\n    if not isinstance(numeric_level, int):\n        raise ValueError(f\"Invalid log level: {level}\")\n    \n    # Check if logger already exists\n    if name in logging.Logger.manager.loggerDict:\n        return logging.getLogger(name)\n    \n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(numeric_level)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(numeric_level)\n    \n    # Create formatter\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers\n    logger.addHandler(console_handler)\n    \n    # Create file handler\n    log_dir = Path(\"logs\")\n    log_dir.mkdir(exist_ok=True)\n    \n    file_handler = logging.FileHandler(log_dir / f\"{name}.log\")\n    file_handler.setLevel(numeric_level)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    \n    return logger"
  },
  {
    "filename": "main.py",
    "language": "Python",
    "content": "import time\nfrom contextlib import asynccontextmanager\nfrom typing import List\n\nimport uvicorn\nfrom fastapi import FastAPI, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\n\nfrom app.api.routes import router as api_router\nfrom app.core.config import Settings, get_settings\nfrom app.utils.logger import get_logger\nimport dotenv\ndotenv.load_dotenv()\nlogger = get_logger(\"main\")\n\n\nasync def validate_azure_settings(settings: Settings) -> bool:\n    \"\"\"Validate Azure settings and report any missing credentials.\"\"\"\n    missing = []\n    \n    # Required Azure OpenAI settings\n    if not settings.AZURE_OPENAI_ENDPOINT:\n        missing.append(\"AZURE_OPENAI_ENDPOINT\")\n    if not settings.AZURE_OPENAI_KEY:\n        missing.append(\"AZURE_OPENAI_KEY\")\n    if not settings.AZURE_OPENAI_EMBEDDING_DEPLOYMENT:\n        missing.append(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n    \n    # Required Azure Search settings\n    if not settings.AZURE_SEARCH_ENDPOINT:\n        missing.append(\"AZURE_SEARCH_ENDPOINT\")\n    if not settings.AZURE_SEARCH_KEY:\n        missing.append(\"AZURE_SEARCH_KEY\")\n    if not settings.AZURE_SEARCH_INDEX_NAME:\n        missing.append(\"AZURE_SEARCH_INDEX_NAME\")\n    \n    if missing:\n        logger.error(f\"Missing required Azure settings: {', '.join(missing)}\")\n        logger.error(\"Please set these environment variables and restart the service\")\n        return False\n    \n    logger.info(\"Azure settings validation passed\")\n    return True\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Startup and shutdown events for the FastAPI app.\"\"\"\n    # Startup\n    logger.info(\"Starting up Juris Query service\")\n    settings = get_settings()\n    \n    # Validate Azure settings\n    if not await validate_azure_settings(settings):\n        logger.error(\"*** SERVICE STARTED WITH INCOMPLETE CONFIGURATION ***\")\n        logger.error(\"The service will run but some functionality will be limited\")\n    \n    # Yield control back to the application\n    yield\n    \n    # Shutdown\n    logger.info(\"Shutting down Juris Query service\")\n\n\ndef create_app() -> FastAPI:\n    \"\"\"Create and configure the FastAPI application.\"\"\"\n    settings = get_settings()\n    \n    app = FastAPI(\n        title=\"Juris Query Service\",\n        description=\"Vector search and query service for Juris legal document analysis\",\n        version=\"0.1.0\",\n        lifespan=lifespan,\n    )\n    \n    # Configure CORS\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=settings.ALLOWED_ORIGINS,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    \n    # Add request ID middleware\n    @app.middleware(\"http\")\n    async def add_request_id(request: Request, call_next):\n        # Add request ID and start time\n        request_id = f\"req_{int(time.time() * 1000)}\"\n        request.state.request_id = request_id\n        request.state.start_time = time.time()\n        \n        try:\n            response = await call_next(request)\n            \n            # Calculate processing time\n            process_time = time.time() - request.state.start_time\n            response.headers[\"X-Process-Time\"] = str(process_time)\n            response.headers[\"X-Request-ID\"] = request_id\n            \n            return response\n        except Exception as e:\n            # Handle any unhandled exceptions\n            logger.error(f\"Unhandled exception: {str(e)}\", extra={\"request_id\": request_id})\n            return JSONResponse(\n                status_code=500,\n                content={\"detail\": \"Internal Server Error\", \"request_id\": request_id},\n            )\n    \n    # Add routes\n    app.include_router(api_router)\n    \n    return app\n\n\napp = create_app()\n\n\nif __name__ == \"__main__\":\n    settings = get_settings()\n    logger.info(f\"Starting server on {settings.API_HOST}:{settings.API_PORT}\")\n    logger.info(f\"Once started, access the API at http://{settings.API_HOST if settings.API_HOST != '0.0.0.0' else 'localhost'}:{settings.API_PORT}/api/health\")\n    logger.info(\"Press Ctrl+C to stop the server\")\n    \n    uvicorn.run(\n        \"app.main:app\",\n        host=settings.API_HOST,\n        port=settings.API_PORT,\n        reload=settings.API_DEBUG,\n    )"
  },
  {
    "filename": "azure_search.log",
    "language": "Unknown",
    "content": "2025-04-11 22:28:23 - azure_search - INFO - Initializing Azure Search service with endpoint: https://jurissearch.search.windows.net\n2025-04-11 22:28:23 - azure_search - INFO - Using index name: 'vector1'\n2025-04-11 22:28:23 - azure_search - INFO - Vector field: vector_content\n2025-04-11 22:28:23 - azure_search - INFO - Text field: content\n2025-04-11 22:28:23 - azure_search - INFO - Searching Azure Search index 'vector1' for top 1 vectors\n2025-04-11 22:28:23 - azure_search - INFO - Using filter: content_metadata/tenant_id eq 'Tenant_1'\n2025-04-11 22:28:24 - azure_search - ERROR - Error querying vector index: HTTP 400\n2025-04-11 22:28:24 - azure_search - ERROR - Response: {\"error\":{\"code\":\"\",\"message\":\"The request is invalid. Details: The parameter 'vectorQueries' in the request payload is not a valid parameter for the operation 'search'.\"}}\n2025-04-11 22:39:32 - azure_search - INFO - Initializing Azure Search service with endpoint: https://jurissearch.search.windows.net\n2025-04-11 22:39:32 - azure_search - INFO - Using index name: 'vector1'\n2025-04-11 22:39:32 - azure_search - INFO - Vector field: vector_content\n2025-04-11 22:39:32 - azure_search - INFO - Text field: content\n2025-04-11 22:39:33 - azure_search - INFO - Searching Azure Search index 'vector1' for top 1 vectors\n2025-04-11 22:39:33 - azure_search - INFO - Using filter: content_metadata/tenant_id eq 'Tenant_1'\n2025-04-11 22:39:34 - azure_search - ERROR - Error querying vector index: HTTP 400\n2025-04-11 22:39:34 - azure_search - ERROR - Response: {\"error\":{\"code\":\"\",\"message\":\"The request is invalid. Details: The parameter 'vectorQueries' in the request payload is not a valid parameter for the operation 'search'.\"}}\n"
  },
  {
    "filename": "README.md",
    "language": "Unknown",
    "content": "# Juris Query Service\n\nA robust, scalable Vector Store & Metadata Index Service for the Juris Legal Document Analysis platform. This service provides a FastAPI application that allows you to query the vector store and retrieve document metadata and content.\n\n## Features\n\n- **Vector Search**: Query for similar documents using state-of-the-art vector embeddings\n- **Metadata Filtering**: Filter results by document metadata (tags, dates, source, etc.)\n- **Document Retrieval**: Get complete documents with chunks and metadata\n- **Azure Cognitive Search Integration**: Uses Azure Cognitive Search as the vector store backend\n- **Performance Optimized**: Fast, efficient queries with minimal latency\n\n## API Endpoints\n\n### POST /api/query\n\nQuery the vector store and get relevant matches.\n\n**Request:**\n```json\n{\n  \"query\": \"What are the key requirements for a valid contract?\",\n  \"top_k\": 5,\n  \"tenant_id\": \"customer1\",\n  \"filters\": [\n    {\n      \"field\": \"tags\",\n      \"value\": [\"contract\", \"legal\"],\n      \"operator\": \"in\"\n    },\n    {\n      \"field\": \"created_at\",\n      \"value\": \"2023-01-01\",\n      \"operator\": \"gte\"\n    }\n  ],\n  \"min_score\": 0.7,\n  \"include_content\": true\n}\n```\n\n**Response:**\n```json\n{\n  \"matches\": [\n    {\n      \"id\": \"chunk123\",\n      \"score\": 0.92,\n      \"content\": \"A valid contract requires offer, acceptance, consideration, legal purpose...\",\n      \"document_id\": \"doc456\",\n      \"chunk_id\": \"chunk123\",\n      \"metadata\": {\n        \"title\": \"Contract Law Basics\",\n        \"author\": \"John Smith\",\n        \"created_at\": \"2023-03-15T14:30:00Z\",\n        \"tags\": [\"contract\", \"legal\", \"requirements\"]\n      }\n    },\n    // More matches...\n  ],\n  \"total_matches\": 5,\n  \"query\": \"What are the key requirements for a valid contract?\",\n  \"query_time_ms\": 45.23\n}\n```\n\n### GET /api/doc/{document_id}\n\nRetrieve a document by its ID, including all chunks and metadata.\n\n**Response:**\n```json\n{\n  \"document_id\": \"doc456\",\n  \"chunks\": [\n    {\n      \"chunk_id\": \"chunk123\",\n      \"document_id\": \"doc456\",\n      \"content\": \"A valid contract requires offer, acceptance, consideration, legal purpose...\",\n      \"chunk_index\": 0,\n      \"metadata\": {\n        \"page\": 1,\n        \"section\": \"Introduction\"\n      }\n    },\n    // More chunks...\n  ],\n  \"metadata\": {\n    \"title\": \"Contract Law Basics\",\n    \"author\": \"John Smith\",\n    \"created_at\": \"2023-03-15T14:30:00Z\",\n    \"tags\": [\"contract\", \"legal\", \"requirements\"]\n  },\n  \"total_chunks\": 5\n}\n```\n\n## Setup\n\n1. Clone the repository\n2. Copy `.env.example` to `.env` and update the configuration values\n3. Install dependencies: `pip install -r requirements.txt`\n4. Run the service: `python -m app.main`\n\n### Configuration\n\nKey environment variables:\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| API_PORT | Port to run the service on | 8002 |\n| AZURE_SEARCH_ENDPOINT | Azure Cognitive Search endpoint | - |\n| AZURE_SEARCH_KEY | Azure Cognitive Search API key | - |\n| AZURE_SEARCH_INDEX_NAME | Azure Cognitive Search index name | vector1 |\n| AZURE_OPENAI_ENDPOINT | Azure OpenAI service endpoint | - |\n| AZURE_OPENAI_KEY | Azure OpenAI API key | - |\n| DEFAULT_TOP_K | Default number of results to return | 5 |\n| MIN_SIMILARITY_SCORE | Minimum similarity score threshold | 0.7 |\n\n## Requirements\n\n- Python 3.8+\n- Azure Cognitive Search account with a vector-enabled index\n- Azure OpenAI account (for query embeddings)\n\n## Architecture\n\nThis service is part of a larger RAG (Retrieval-Augmented Generation) pipeline:\n\n1. **Document Ingestion**: Upload and process documents (Stage 1)\n2. **Vectorization**: Generate embeddings for document chunks (Stage 2)\n3. **Vector Store & Query**: Search and retrieve relevant content (Stage 3, this service)\n4. **Orchestration & LLM Integration**: Use retrieved content with LLMs (Stage 4)\n5. **Authentication & Security**: Add authentication (Stage 5)\n\n## Development\n\n- Use `API_DEBUG=true` for hot reloading during development\n- Check logs in the `logs` directory\n- Run health check: `curl http://localhost:8002/api/health`"
  }
]